Copyright (c) Imply Data, Inc. All rights reserved.

This software is the confidential and proprietary information
of Imply Data, Inc. You shall not disclose such Confidential
Information and shall use it only in accordance with the terms
of the license agreement you entered into with Imply.
==============================================================
Test cases from the CalciteInsertDmlTest file

Includes the actual "execution plan" information for Talaria,
which here means the controller task definition.

These tests omit the "native" section: the query that would
be listed in that section is identical to the one embedded
in the Talaria "execPlan."
==============================================================
Converted from testInsertFromTable()
=== case
INSERT from table
=== SQL
INSERT INTO dst SELECT * FROM foo PARTITIONED BY ALL TIME
=== schema
TASK VARCHAR
=== targetSchema
__time TIMESTAMP(3)
cnt BIGINT
dim1 VARCHAR
dim2 VARCHAR
dim3 VARCHAR
m1 FLOAT
m2 DOUBLE
unique_dim1 COMPLEX<hyperUnique>
=== resources
DATASOURCE/dst/WRITE
DATASOURCE/foo/READ
=== plan
LogicalInsert(target=[dst], granularity=[AllGranularity])
  LogicalProject(__time=[$0], cnt=[$1], dim1=[$2], dim2=[$3], dim3=[$4], m1=[$5], m2=[$6], unique_dim1=[$7])
    LogicalTableScan(table=[[druid, foo]])
=== execPlan
{
  "type" : "query_controller",
  "id" : "query-dummyId",
  "spec" : {
    "query" : {
      "queryType" : "scan",
      "dataSource" : {
        "type" : "table",
        "name" : "foo"
      },
      "intervals" : {
        "type" : "intervals",
        "intervals" : [ "-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z" ]
      },
      "resultFormat" : "compactedList",
      "columns" : [ "__time", "cnt", "dim1", "dim2", "dim3", "m1", "m2", "unique_dim1" ],
      "legacy" : false,
      "context" : {
        "finalize" : true,
        "msqSignature" : "[{\"name\":\"__time\",\"type\":\"LONG\"},{\"name\":\"cnt\",\"type\":\"LONG\"},{\"name\":\"dim1\",\"type\":\"STRING\"},{\"name\":\"dim2\",\"type\":\"STRING\"},{\"name\":\"dim3\",\"type\":\"STRING\"},{\"name\":\"m1\",\"type\":\"FLOAT\"},{\"name\":\"m2\",\"type\":\"DOUBLE\"},{\"name\":\"unique_dim1\",\"type\":\"COMPLEX<hyperUnique>\"}]",
        "msqTimeColumn" : "__time",
        "multiStageQuery" : true,
        "sqlInsertSegmentGranularity" : "{\"type\":\"all\"}",
        "sqlQueryId" : "dummyId"
      },
      "granularity" : {
        "type" : "all"
      }
    },
    "columnMappings" : [ {
      "queryColumn" : "__time",
      "outputColumn" : "__time"
    }, {
      "queryColumn" : "cnt",
      "outputColumn" : "cnt"
    }, {
      "queryColumn" : "dim1",
      "outputColumn" : "dim1"
    }, {
      "queryColumn" : "dim2",
      "outputColumn" : "dim2"
    }, {
      "queryColumn" : "dim3",
      "outputColumn" : "dim3"
    }, {
      "queryColumn" : "m1",
      "outputColumn" : "m1"
    }, {
      "queryColumn" : "m2",
      "outputColumn" : "m2"
    }, {
      "queryColumn" : "unique_dim1",
      "outputColumn" : "unique_dim1"
    } ],
    "destination" : {
      "type" : "dataSource",
      "dataSource" : "dst",
      "segmentGranularity" : {
        "type" : "all"
      }
    },
    "tuningConfig" : {
      "type" : "index_parallel",
      "partitionsSpec" : {
        "type" : "dynamic",
        "maxRowsPerSegment" : 3000000,
        "maxTotalRows" : null
      },
      "maxNumConcurrentSubTasks" : 1,
      "maxRetry" : 1,
      "maxRowsInMemory" : 100000
    }
  },
  "sqlQuery" : "INSERT INTO dst SELECT * FROM foo PARTITIONED BY ALL TIME",
  "sqlQueryContext" : {
    "maxParseExceptions" : 0,
    "msqSignature" : "[{\"name\":\"__time\",\"type\":\"LONG\"},{\"name\":\"cnt\",\"type\":\"LONG\"},{\"name\":\"dim1\",\"type\":\"STRING\"},{\"name\":\"dim2\",\"type\":\"STRING\"},{\"name\":\"dim3\",\"type\":\"STRING\"},{\"name\":\"m1\",\"type\":\"FLOAT\"},{\"name\":\"m2\",\"type\":\"DOUBLE\"},{\"name\":\"unique_dim1\",\"type\":\"COMPLEX<hyperUnique>\"}]",
    "multiStageQuery" : true,
    "sqlInsertSegmentGranularity" : "{\"type\":\"all\"}",
    "sqlQueryId" : "dummyId"
  },
  "sqlTypeNames" : [ "TIMESTAMP", "BIGINT", "VARCHAR", "VARCHAR", "VARCHAR", "FLOAT", "DOUBLE", "OTHER" ]
}
==============================================================
Converted from testInsertFromView()
=== case
INSERT from view
=== SQL
INSERT INTO dst SELECT * FROM view.aview PARTITIONED BY ALL TIME
=== schema
TASK VARCHAR
=== targetSchema
dim1_firstchar VARCHAR
=== resources
DATASOURCE/dst/WRITE
VIEW/aview/READ
=== plan
LogicalInsert(target=[dst], granularity=[AllGranularity])
  LogicalProject(dim1_firstchar=[SUBSTRING($2, 1, 1)])
    LogicalFilter(condition=[=($3, 'a')])
      LogicalTableScan(table=[[druid, foo]])
=== execPlan
{
  "type" : "query_controller",
  "id" : "query-dummyId",
  "spec" : {
    "query" : {
      "queryType" : "scan",
      "dataSource" : {
        "type" : "table",
        "name" : "foo"
      },
      "intervals" : {
        "type" : "intervals",
        "intervals" : [ "-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z" ]
      },
      "virtualColumns" : [ {
        "type" : "expression",
        "name" : "v0",
        "expression" : "substring(\"dim1\", 0, 1)",
        "outputType" : "STRING"
      } ],
      "resultFormat" : "compactedList",
      "filter" : {
        "type" : "selector",
        "dimension" : "dim2",
        "value" : "a"
      },
      "columns" : [ "v0" ],
      "legacy" : false,
      "context" : {
        "finalize" : true,
        "msqSignature" : "[{\"name\":\"v0\",\"type\":\"STRING\"}]",
        "multiStageQuery" : true,
        "sqlInsertSegmentGranularity" : "{\"type\":\"all\"}",
        "sqlQueryId" : "dummyId"
      },
      "granularity" : {
        "type" : "all"
      }
    },
    "columnMappings" : [ {
      "queryColumn" : "v0",
      "outputColumn" : "dim1_firstchar"
    } ],
    "destination" : {
      "type" : "dataSource",
      "dataSource" : "dst",
      "segmentGranularity" : {
        "type" : "all"
      }
    },
    "tuningConfig" : {
      "type" : "index_parallel",
      "partitionsSpec" : {
        "type" : "dynamic",
        "maxRowsPerSegment" : 3000000,
        "maxTotalRows" : null
      },
      "maxNumConcurrentSubTasks" : 1,
      "maxRetry" : 1,
      "maxRowsInMemory" : 100000
    }
  },
  "sqlQuery" : "INSERT INTO dst SELECT * FROM view.aview PARTITIONED BY ALL TIME",
  "sqlQueryContext" : {
    "maxParseExceptions" : 0,
    "msqSignature" : "[{\"name\":\"v0\",\"type\":\"STRING\"}]",
    "multiStageQuery" : true,
    "sqlInsertSegmentGranularity" : "{\"type\":\"all\"}",
    "sqlQueryId" : "dummyId"
  },
  "sqlTypeNames" : [ "VARCHAR" ]
}
==============================================================
Converted from testInsertIntoExistingTable()
=== case
INSERT into existing table
=== SQL
INSERT INTO foo SELECT * FROM foo PARTITIONED BY ALL TIME
=== schema
TASK VARCHAR
=== targetSchema
__time TIMESTAMP(3)
cnt BIGINT
dim1 VARCHAR
dim2 VARCHAR
dim3 VARCHAR
m1 FLOAT
m2 DOUBLE
unique_dim1 COMPLEX<hyperUnique>
=== resources
DATASOURCE/foo/READ
DATASOURCE/foo/WRITE
=== plan
LogicalInsert(target=[foo], granularity=[AllGranularity])
  LogicalProject(__time=[$0], cnt=[$1], dim1=[$2], dim2=[$3], dim3=[$4], m1=[$5], m2=[$6], unique_dim1=[$7])
    LogicalTableScan(table=[[druid, foo]])
=== execPlan
{
  "type" : "query_controller",
  "id" : "query-dummyId",
  "spec" : {
    "query" : {
      "queryType" : "scan",
      "dataSource" : {
        "type" : "table",
        "name" : "foo"
      },
      "intervals" : {
        "type" : "intervals",
        "intervals" : [ "-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z" ]
      },
      "resultFormat" : "compactedList",
      "columns" : [ "__time", "cnt", "dim1", "dim2", "dim3", "m1", "m2", "unique_dim1" ],
      "legacy" : false,
      "context" : {
        "finalize" : true,
        "msqSignature" : "[{\"name\":\"__time\",\"type\":\"LONG\"},{\"name\":\"cnt\",\"type\":\"LONG\"},{\"name\":\"dim1\",\"type\":\"STRING\"},{\"name\":\"dim2\",\"type\":\"STRING\"},{\"name\":\"dim3\",\"type\":\"STRING\"},{\"name\":\"m1\",\"type\":\"FLOAT\"},{\"name\":\"m2\",\"type\":\"DOUBLE\"},{\"name\":\"unique_dim1\",\"type\":\"COMPLEX<hyperUnique>\"}]",
        "msqTimeColumn" : "__time",
        "multiStageQuery" : true,
        "sqlInsertSegmentGranularity" : "{\"type\":\"all\"}",
        "sqlQueryId" : "dummyId"
      },
      "granularity" : {
        "type" : "all"
      }
    },
    "columnMappings" : [ {
      "queryColumn" : "__time",
      "outputColumn" : "__time"
    }, {
      "queryColumn" : "cnt",
      "outputColumn" : "cnt"
    }, {
      "queryColumn" : "dim1",
      "outputColumn" : "dim1"
    }, {
      "queryColumn" : "dim2",
      "outputColumn" : "dim2"
    }, {
      "queryColumn" : "dim3",
      "outputColumn" : "dim3"
    }, {
      "queryColumn" : "m1",
      "outputColumn" : "m1"
    }, {
      "queryColumn" : "m2",
      "outputColumn" : "m2"
    }, {
      "queryColumn" : "unique_dim1",
      "outputColumn" : "unique_dim1"
    } ],
    "destination" : {
      "type" : "dataSource",
      "dataSource" : "foo",
      "segmentGranularity" : {
        "type" : "all"
      }
    },
    "tuningConfig" : {
      "type" : "index_parallel",
      "partitionsSpec" : {
        "type" : "dynamic",
        "maxRowsPerSegment" : 3000000,
        "maxTotalRows" : null
      },
      "maxNumConcurrentSubTasks" : 1,
      "maxRetry" : 1,
      "maxRowsInMemory" : 100000
    }
  },
  "sqlQuery" : "INSERT INTO foo SELECT * FROM foo PARTITIONED BY ALL TIME",
  "sqlQueryContext" : {
    "maxParseExceptions" : 0,
    "msqSignature" : "[{\"name\":\"__time\",\"type\":\"LONG\"},{\"name\":\"cnt\",\"type\":\"LONG\"},{\"name\":\"dim1\",\"type\":\"STRING\"},{\"name\":\"dim2\",\"type\":\"STRING\"},{\"name\":\"dim3\",\"type\":\"STRING\"},{\"name\":\"m1\",\"type\":\"FLOAT\"},{\"name\":\"m2\",\"type\":\"DOUBLE\"},{\"name\":\"unique_dim1\",\"type\":\"COMPLEX<hyperUnique>\"}]",
    "multiStageQuery" : true,
    "sqlInsertSegmentGranularity" : "{\"type\":\"all\"}",
    "sqlQueryId" : "dummyId"
  },
  "sqlTypeNames" : [ "TIMESTAMP", "BIGINT", "VARCHAR", "VARCHAR", "VARCHAR", "FLOAT", "DOUBLE", "OTHER" ]
}
==============================================================
Converted from testInsertIntoQualifiedTable()
=== case
INSERT from view
=== SQL
INSERT INTO druid.dst SELECT * FROM foo PARTITIONED BY ALL TIME
=== schema
TASK VARCHAR
=== targetSchema
__time TIMESTAMP(3)
cnt BIGINT
dim1 VARCHAR
dim2 VARCHAR
dim3 VARCHAR
m1 FLOAT
m2 DOUBLE
unique_dim1 COMPLEX<hyperUnique>
=== resources
DATASOURCE/dst/WRITE
DATASOURCE/foo/READ
=== plan
LogicalInsert(target=[druid.dst], granularity=[AllGranularity])
  LogicalProject(__time=[$0], cnt=[$1], dim1=[$2], dim2=[$3], dim3=[$4], m1=[$5], m2=[$6], unique_dim1=[$7])
    LogicalTableScan(table=[[druid, foo]])
=== execPlan
{
  "type" : "query_controller",
  "id" : "query-dummyId",
  "spec" : {
    "query" : {
      "queryType" : "scan",
      "dataSource" : {
        "type" : "table",
        "name" : "foo"
      },
      "intervals" : {
        "type" : "intervals",
        "intervals" : [ "-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z" ]
      },
      "resultFormat" : "compactedList",
      "columns" : [ "__time", "cnt", "dim1", "dim2", "dim3", "m1", "m2", "unique_dim1" ],
      "legacy" : false,
      "context" : {
        "finalize" : true,
        "msqSignature" : "[{\"name\":\"__time\",\"type\":\"LONG\"},{\"name\":\"cnt\",\"type\":\"LONG\"},{\"name\":\"dim1\",\"type\":\"STRING\"},{\"name\":\"dim2\",\"type\":\"STRING\"},{\"name\":\"dim3\",\"type\":\"STRING\"},{\"name\":\"m1\",\"type\":\"FLOAT\"},{\"name\":\"m2\",\"type\":\"DOUBLE\"},{\"name\":\"unique_dim1\",\"type\":\"COMPLEX<hyperUnique>\"}]",
        "msqTimeColumn" : "__time",
        "multiStageQuery" : true,
        "sqlInsertSegmentGranularity" : "{\"type\":\"all\"}",
        "sqlQueryId" : "dummyId"
      },
      "granularity" : {
        "type" : "all"
      }
    },
    "columnMappings" : [ {
      "queryColumn" : "__time",
      "outputColumn" : "__time"
    }, {
      "queryColumn" : "cnt",
      "outputColumn" : "cnt"
    }, {
      "queryColumn" : "dim1",
      "outputColumn" : "dim1"
    }, {
      "queryColumn" : "dim2",
      "outputColumn" : "dim2"
    }, {
      "queryColumn" : "dim3",
      "outputColumn" : "dim3"
    }, {
      "queryColumn" : "m1",
      "outputColumn" : "m1"
    }, {
      "queryColumn" : "m2",
      "outputColumn" : "m2"
    }, {
      "queryColumn" : "unique_dim1",
      "outputColumn" : "unique_dim1"
    } ],
    "destination" : {
      "type" : "dataSource",
      "dataSource" : "dst",
      "segmentGranularity" : {
        "type" : "all"
      }
    },
    "tuningConfig" : {
      "type" : "index_parallel",
      "partitionsSpec" : {
        "type" : "dynamic",
        "maxRowsPerSegment" : 3000000,
        "maxTotalRows" : null
      },
      "maxNumConcurrentSubTasks" : 1,
      "maxRetry" : 1,
      "maxRowsInMemory" : 100000
    }
  },
  "sqlQuery" : "INSERT INTO druid.dst SELECT * FROM foo PARTITIONED BY ALL TIME",
  "sqlQueryContext" : {
    "maxParseExceptions" : 0,
    "msqSignature" : "[{\"name\":\"__time\",\"type\":\"LONG\"},{\"name\":\"cnt\",\"type\":\"LONG\"},{\"name\":\"dim1\",\"type\":\"STRING\"},{\"name\":\"dim2\",\"type\":\"STRING\"},{\"name\":\"dim3\",\"type\":\"STRING\"},{\"name\":\"m1\",\"type\":\"FLOAT\"},{\"name\":\"m2\",\"type\":\"DOUBLE\"},{\"name\":\"unique_dim1\",\"type\":\"COMPLEX<hyperUnique>\"}]",
    "multiStageQuery" : true,
    "sqlInsertSegmentGranularity" : "{\"type\":\"all\"}",
    "sqlQueryId" : "dummyId"
  },
  "sqlTypeNames" : [ "TIMESTAMP", "BIGINT", "VARCHAR", "VARCHAR", "VARCHAR", "FLOAT", "DOUBLE", "OTHER" ]
}
==============================================================
Converted from testInsertIntoInvalidDataSourceName()
=== case
INSERT into invalid datasource name
=== SQL
INSERT INTO "in/valid" SELECT dim1, dim2 FROM foo PARTITIONED BY ALL TIME
=== exception
ValidationException
=== error
INSERT dataSource cannot contain the '/' character.
==============================================================
Converted from testInsertUsingColumnList()
=== case
INSERT using column list
=== SQL
INSERT INTO dst (foo, bar) SELECT dim1, dim2 FROM foo PARTITIONED BY ALL TIME
=== exception
ValidationException
=== error
INSERT with target column list is not supported.
==============================================================
Converted from testUpsert()
=== case
Upsert
=== SQL
UPSERT INTO dst SELECT * FROM foo PARTITIONED BY ALL TIME
=== exception
ValidationException
=== error
UPSERT is not supported.
==============================================================
Converted from testInsertIntoSystemTable()
=== case
INSERT into system table
=== SQL
INSERT INTO INFORMATION_SCHEMA.COLUMNS SELECT * FROM foo PARTITIONED BY ALL TIME
=== exception
ValidationException
=== error
Cannot INSERT into [INFORMATION_SCHEMA.COLUMNS] because it is not a Druid datasource.
==============================================================
Converted from testInsertIntoView()
=== case
INSERT into view
=== SQL
INSERT INTO view.aview SELECT * FROM foo PARTITIONED BY ALL TIME
=== exception
ValidationException
=== error
Cannot INSERT into [view.aview] because it is not a Druid datasource.
==============================================================
Not converted here because the planner does not enforce security:
* testInsertFromUnauthorizedDataSource()
* testInsertIntoUnauthorizedDataSource()
==============================================================
Converted from testInsertIntoNonexistentSchema()
=== case
INSERT into nonexistent schema
=== SQL
INSERT INTO nonexistent.dst SELECT * FROM foo PARTITIONED BY ALL TIME
=== exception
ValidationException
=== error
Cannot INSERT into [nonexistent.dst] because it is not a Druid datasource.
==============================================================
Converted from testInsertFromExternal()
=== case
INSERT from external
=== SQL
INSERT INTO dst SELECT *
FROM TABLE(extern(
   '{
     "type": "inline",
     "data": "a,b,1\nc,d,2\n"
    }',
	'{
	  "type": "csv",
	  "columns": ["x","y","z"],
	  "listDelimiter": null,
	  "findColumnsFromHeader": false,
	  "skipHeaderRows": 0
	 }',
	 '[
	   {"name": "x", "type": "STRING"},
	   {"name": "y", "type": "STRING"},
	   {"name": "z", "type": "LONG"}
	 ]'
))
PARTITIONED BY ALL TIME
=== schema
TASK VARCHAR
=== targetSchema
x VARCHAR
y VARCHAR
z BIGINT
=== resources
DATASOURCE/dst/WRITE
EXTERNAL/EXTERNAL/READ
=== plan
LogicalInsert(target=[dst], granularity=[AllGranularity])
  LogicalProject(x=[$0], y=[$1], z=[$2])
    ExternalTableScan(dataSource=[{"type":"external","inputSource":{"type":"inline","data":"a,b,1\nc,d,2\n"},"inputFormat":{"type":"csv","columns":["x","y","z"],"listDelimiter":null,"findColumnsFromHeader":false,"skipHeaderRows":0},"signature":[{"name":"x","type":"STRING"},{"name":"y","type":"STRING"},{"name":"z","type":"LONG"}]}])
=== execPlan
{
  "type" : "query_controller",
  "id" : "query-dummyId",
  "spec" : {
    "query" : {
      "queryType" : "scan",
      "dataSource" : {
        "type" : "external",
        "inputSource" : {
          "type" : "inline",
          "data" : "a,b,1\nc,d,2\n"
        },
        "inputFormat" : {
          "type" : "csv",
          "columns" : [ "x", "y", "z" ],
          "listDelimiter" : null,
          "findColumnsFromHeader" : false,
          "skipHeaderRows" : 0
        },
        "signature" : [ {
          "name" : "x",
          "type" : "STRING"
        }, {
          "name" : "y",
          "type" : "STRING"
        }, {
          "name" : "z",
          "type" : "LONG"
        } ]
      },
      "intervals" : {
        "type" : "intervals",
        "intervals" : [ "-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z" ]
      },
      "resultFormat" : "compactedList",
      "columns" : [ "x", "y", "z" ],
      "legacy" : false,
      "context" : {
        "finalize" : true,
        "msqSignature" : "[{\"name\":\"x\",\"type\":\"STRING\"},{\"name\":\"y\",\"type\":\"STRING\"},{\"name\":\"z\",\"type\":\"LONG\"}]",
        "multiStageQuery" : true,
        "sqlInsertSegmentGranularity" : "{\"type\":\"all\"}",
        "sqlQueryId" : "dummyId"
      },
      "granularity" : {
        "type" : "all"
      }
    },
    "columnMappings" : [ {
      "queryColumn" : "x",
      "outputColumn" : "x"
    }, {
      "queryColumn" : "y",
      "outputColumn" : "y"
    }, {
      "queryColumn" : "z",
      "outputColumn" : "z"
    } ],
    "destination" : {
      "type" : "dataSource",
      "dataSource" : "dst",
      "segmentGranularity" : {
        "type" : "all"
      }
    },
    "tuningConfig" : {
      "type" : "index_parallel",
      "partitionsSpec" : {
        "type" : "dynamic",
        "maxRowsPerSegment" : 3000000,
        "maxTotalRows" : null
      },
      "maxNumConcurrentSubTasks" : 1,
      "maxRetry" : 1,
      "maxRowsInMemory" : 100000
    }
  },
  "sqlQuery" : "INSERT INTO dst SELECT *\nFROM TABLE(extern(\n   '{\n     \"type\": \"inline\",\n     \"data\": \"a,b,1\\nc,d,2\\n\"\n    }',\n\t'{\n\t  \"type\": \"csv\",\n\t  \"columns\": [\"x\",\"y\",\"z\"],\n\t  \"listDelimiter\": null,\n\t  \"findColumnsFromHeader\": false,\n\t  \"skipHeaderRows\": 0\n\t }',\n\t '[\n\t   {\"name\": \"x\", \"type\": \"STRING\"},\n\t   {\"name\": \"y\", \"type\": \"STRING\"},\n\t   {\"name\": \"z\", \"type\": \"LONG\"}\n\t ]'\n))\nPARTITIONED BY ALL TIME",
  "sqlQueryContext" : {
    "maxParseExceptions" : 0,
    "msqSignature" : "[{\"name\":\"x\",\"type\":\"STRING\"},{\"name\":\"y\",\"type\":\"STRING\"},{\"name\":\"z\",\"type\":\"LONG\"}]",
    "multiStageQuery" : true,
    "sqlInsertSegmentGranularity" : "{\"type\":\"all\"}",
    "sqlQueryId" : "dummyId"
  },
  "sqlTypeNames" : [ "VARCHAR", "VARCHAR", "BIGINT" ]
}
==============================================================
As above, but use an "input table" defined exactly the same
as the extern table.
=== case
INSERT from external
=== SQL
INSERT INTO dst SELECT *
FROM "input"."inline"
PARTITIONED BY ALL TIME
=== plan copy
=== targetSchema copy
=== resources
DATASOURCE/dst/WRITE
INPUT/inline/READ
=== execPlan copy
==============================================================
Converted from testInsertWithPartitionedBy()
=== case
INSERT with PARTITIONED BY
=== SQL
INSERT INTO druid.dst
SELECT
  __time,
  FLOOR(m1) as floor_m1,
  dim1 FROM foo
PARTITIONED BY TIME_FLOOR(__time, 'PT1H')
=== schema
TASK VARCHAR
=== targetSchema
__time TIMESTAMP(3)
floor_m1 FLOAT
dim1 VARCHAR
=== resources
DATASOURCE/dst/WRITE
DATASOURCE/foo/READ
=== plan
LogicalInsert(target=[druid.dst], granularity=[{type=period, period=PT1H, timeZone=UTC, origin=null}])
  LogicalProject(__time=[$0], floor_m1=[FLOOR($5)], dim1=[$2])
    LogicalTableScan(table=[[druid, foo]])
=== execPlan
{
  "type" : "query_controller",
  "id" : "query-dummyId",
  "spec" : {
    "query" : {
      "queryType" : "scan",
      "dataSource" : {
        "type" : "table",
        "name" : "foo"
      },
      "intervals" : {
        "type" : "intervals",
        "intervals" : [ "-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z" ]
      },
      "virtualColumns" : [ {
        "type" : "expression",
        "name" : "v0",
        "expression" : "floor(\"m1\")",
        "outputType" : "FLOAT"
      } ],
      "resultFormat" : "compactedList",
      "columns" : [ "__time", "dim1", "v0" ],
      "legacy" : false,
      "context" : {
        "finalize" : true,
        "msqSignature" : "[{\"name\":\"__time\",\"type\":\"LONG\"},{\"name\":\"dim1\",\"type\":\"STRING\"},{\"name\":\"v0\",\"type\":\"FLOAT\"}]",
        "msqTimeColumn" : "__time",
        "multiStageQuery" : true,
        "sqlInsertSegmentGranularity" : "\"HOUR\"",
        "sqlQueryId" : "dummyId"
      },
      "granularity" : {
        "type" : "all"
      }
    },
    "columnMappings" : [ {
      "queryColumn" : "__time",
      "outputColumn" : "__time"
    }, {
      "queryColumn" : "v0",
      "outputColumn" : "floor_m1"
    }, {
      "queryColumn" : "dim1",
      "outputColumn" : "dim1"
    } ],
    "destination" : {
      "type" : "dataSource",
      "dataSource" : "dst",
      "segmentGranularity" : "HOUR"
    },
    "tuningConfig" : {
      "type" : "index_parallel",
      "partitionsSpec" : {
        "type" : "dynamic",
        "maxRowsPerSegment" : 3000000,
        "maxTotalRows" : null
      },
      "maxNumConcurrentSubTasks" : 1,
      "maxRetry" : 1,
      "maxRowsInMemory" : 100000
    }
  },
  "sqlQuery" : "INSERT INTO druid.dst\nSELECT\n  __time,\n  FLOOR(m1) as floor_m1,\n  dim1 FROM foo\nPARTITIONED BY TIME_FLOOR(__time, 'PT1H')",
  "sqlQueryContext" : {
    "maxParseExceptions" : 0,
    "msqSignature" : "[{\"name\":\"__time\",\"type\":\"LONG\"},{\"name\":\"dim1\",\"type\":\"STRING\"},{\"name\":\"v0\",\"type\":\"FLOAT\"}]",
    "multiStageQuery" : true,
    "sqlInsertSegmentGranularity" : "\"HOUR\"",
    "sqlQueryId" : "dummyId"
  },
  "sqlTypeNames" : [ "TIMESTAMP", "FLOAT", "VARCHAR" ]
}
==============================================================
Not converted because the the code form iterates over many
variations:
* testPartitionedBySupportedClauses
==============================================================
Converted from testInsertWithClusteredBy()
=== case
INSERT with CLUSTERED BY
=== SQL
INSERT INTO druid.dst
SELECT
    __time,
    FLOOR(m1) as floor_m1,
    dim1, CEIL(m2) as ceil_m2
FROM foo
PARTITIONED BY FLOOR(__time TO DAY)
CLUSTERED BY 2, dim1 DESC, CEIL(m2)
=== schema
TASK VARCHAR
=== targetSchema
__time TIMESTAMP(3)
floor_m1 FLOAT
dim1 VARCHAR
ceil_m2 DOUBLE
=== resources
DATASOURCE/dst/WRITE
DATASOURCE/foo/READ
=== plan
LogicalInsert(target=[druid.dst], granularity=[{type=period, period=P1D, timeZone=UTC, origin=null}])
  Clustered By: 2, `dim1` DESC, CEIL(`m2`)  LogicalSort(sort0=[$1], sort1=[$2], sort2=[$3], dir0=[ASC], dir1=[DESC], dir2=[ASC])
    LogicalProject(__time=[$0], floor_m1=[FLOOR($5)], dim1=[$2], ceil_m2=[CEIL($6)])
      LogicalTableScan(table=[[druid, foo]])
=== execPlan
{
  "type" : "query_controller",
  "id" : "query-dummyId",
  "spec" : {
    "query" : {
      "queryType" : "scan",
      "dataSource" : {
        "type" : "table",
        "name" : "foo"
      },
      "intervals" : {
        "type" : "intervals",
        "intervals" : [ "-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z" ]
      },
      "virtualColumns" : [ {
        "type" : "expression",
        "name" : "v0",
        "expression" : "floor(\"m1\")",
        "outputType" : "FLOAT"
      }, {
        "type" : "expression",
        "name" : "v1",
        "expression" : "ceil(\"m2\")",
        "outputType" : "DOUBLE"
      } ],
      "resultFormat" : "compactedList",
      "orderBy" : [ {
        "columnName" : "v0",
        "order" : "ascending"
      }, {
        "columnName" : "dim1",
        "order" : "descending"
      }, {
        "columnName" : "v1",
        "order" : "ascending"
      } ],
      "columns" : [ "__time", "dim1", "v0", "v1" ],
      "legacy" : false,
      "context" : {
        "finalize" : true,
        "msqSignature" : "[{\"name\":\"__time\",\"type\":\"LONG\"},{\"name\":\"dim1\",\"type\":\"STRING\"},{\"name\":\"v0\",\"type\":\"FLOAT\"},{\"name\":\"v1\",\"type\":\"DOUBLE\"}]",
        "msqTimeColumn" : "__time",
        "multiStageQuery" : true,
        "sqlInsertSegmentGranularity" : "\"DAY\"",
        "sqlQueryId" : "dummyId"
      },
      "granularity" : {
        "type" : "all"
      }
    },
    "columnMappings" : [ {
      "queryColumn" : "__time",
      "outputColumn" : "__time"
    }, {
      "queryColumn" : "v0",
      "outputColumn" : "floor_m1"
    }, {
      "queryColumn" : "dim1",
      "outputColumn" : "dim1"
    }, {
      "queryColumn" : "v1",
      "outputColumn" : "ceil_m2"
    } ],
    "destination" : {
      "type" : "dataSource",
      "dataSource" : "dst",
      "segmentGranularity" : "DAY"
    },
    "tuningConfig" : {
      "type" : "index_parallel",
      "partitionsSpec" : {
        "type" : "dynamic",
        "maxRowsPerSegment" : 3000000,
        "maxTotalRows" : null
      },
      "maxNumConcurrentSubTasks" : 1,
      "maxRetry" : 1,
      "maxRowsInMemory" : 100000
    }
  },
  "sqlQuery" : "INSERT INTO druid.dst\nSELECT\n    __time,\n    FLOOR(m1) as floor_m1,\n    dim1, CEIL(m2) as ceil_m2\nFROM foo\nPARTITIONED BY FLOOR(__time TO DAY)\nCLUSTERED BY 2, dim1 DESC, CEIL(m2)",
  "sqlQueryContext" : {
    "maxParseExceptions" : 0,
    "msqSignature" : "[{\"name\":\"__time\",\"type\":\"LONG\"},{\"name\":\"dim1\",\"type\":\"STRING\"},{\"name\":\"v0\",\"type\":\"FLOAT\"},{\"name\":\"v1\",\"type\":\"DOUBLE\"}]",
    "multiStageQuery" : true,
    "sqlInsertSegmentGranularity" : "\"DAY\"",
    "sqlQueryId" : "dummyId"
  },
  "sqlTypeNames" : [ "TIMESTAMP", "FLOAT", "VARCHAR", "DOUBLE" ]
}
==============================================================
Converted from testInsertWithPartitionedByAndClusteredBy()
=== case
INSERT with PARTITIONED BY and CLUSTERED BY
=== SQL
INSERT INTO druid.dst
SELECT
    __time,
    FLOOR(m1) as floor_m1,
    dim1 FROM foo
PARTITIONED BY DAY
CLUSTERED BY 2, dim1
=== schema
TASK VARCHAR
=== targetSchema
__time TIMESTAMP(3)
floor_m1 FLOAT
dim1 VARCHAR
=== resources
DATASOURCE/dst/WRITE
DATASOURCE/foo/READ
=== plan
LogicalInsert(target=[druid.dst], granularity=[{type=period, period=P1D, timeZone=UTC, origin=null}])
  Clustered By: 2, `dim1`  LogicalSort(sort0=[$1], sort1=[$2], dir0=[ASC], dir1=[ASC])
    LogicalProject(__time=[$0], floor_m1=[FLOOR($5)], dim1=[$2])
      LogicalTableScan(table=[[druid, foo]])
=== execPlan
{
  "type" : "query_controller",
  "id" : "query-dummyId",
  "spec" : {
    "query" : {
      "queryType" : "scan",
      "dataSource" : {
        "type" : "table",
        "name" : "foo"
      },
      "intervals" : {
        "type" : "intervals",
        "intervals" : [ "-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z" ]
      },
      "virtualColumns" : [ {
        "type" : "expression",
        "name" : "v0",
        "expression" : "floor(\"m1\")",
        "outputType" : "FLOAT"
      } ],
      "resultFormat" : "compactedList",
      "orderBy" : [ {
        "columnName" : "v0",
        "order" : "ascending"
      }, {
        "columnName" : "dim1",
        "order" : "ascending"
      } ],
      "columns" : [ "__time", "dim1", "v0" ],
      "legacy" : false,
      "context" : {
        "finalize" : true,
        "msqSignature" : "[{\"name\":\"__time\",\"type\":\"LONG\"},{\"name\":\"dim1\",\"type\":\"STRING\"},{\"name\":\"v0\",\"type\":\"FLOAT\"}]",
        "msqTimeColumn" : "__time",
        "multiStageQuery" : true,
        "sqlInsertSegmentGranularity" : "\"DAY\"",
        "sqlQueryId" : "dummyId"
      },
      "granularity" : {
        "type" : "all"
      }
    },
    "columnMappings" : [ {
      "queryColumn" : "__time",
      "outputColumn" : "__time"
    }, {
      "queryColumn" : "v0",
      "outputColumn" : "floor_m1"
    }, {
      "queryColumn" : "dim1",
      "outputColumn" : "dim1"
    } ],
    "destination" : {
      "type" : "dataSource",
      "dataSource" : "dst",
      "segmentGranularity" : "DAY"
    },
    "tuningConfig" : {
      "type" : "index_parallel",
      "partitionsSpec" : {
        "type" : "dynamic",
        "maxRowsPerSegment" : 3000000,
        "maxTotalRows" : null
      },
      "maxNumConcurrentSubTasks" : 1,
      "maxRetry" : 1,
      "maxRowsInMemory" : 100000
    }
  },
  "sqlQuery" : "INSERT INTO druid.dst\nSELECT\n    __time,\n    FLOOR(m1) as floor_m1,\n    dim1 FROM foo\nPARTITIONED BY DAY\nCLUSTERED BY 2, dim1",
  "sqlQueryContext" : {
    "maxParseExceptions" : 0,
    "msqSignature" : "[{\"name\":\"__time\",\"type\":\"LONG\"},{\"name\":\"dim1\",\"type\":\"STRING\"},{\"name\":\"v0\",\"type\":\"FLOAT\"}]",
    "multiStageQuery" : true,
    "sqlInsertSegmentGranularity" : "\"DAY\"",
    "sqlQueryId" : "dummyId"
  },
  "sqlTypeNames" : [ "TIMESTAMP", "FLOAT", "VARCHAR" ]
}
==============================================================
Converted from testInsertWithPartitionedByAndLimitOffset()

This test passes in the Druid "mock" implementation of INSERT
but fails in Talaria due to technical limitations.
=== case
INSERT with PARTITIONED BY and LIMIT OFFSET
=== SQL
INSERT INTO druid.dst
SELECT
    __time,
    FLOOR(m1) as floor_m1,
    dim1 FROM foo
LIMIT 10
OFFSET 20
PARTITIONED BY DAY
=== exception
ValidationException
=== error
INSERT queries cannot end with LIMIT unless sqlInsertSegmentGranularity is "all".
==============================================================
Converted from testInsertWithClusteredByAndOrderBy()
=== case
INSERT with CLUSTERED BY and ORDER BY
=== SQL
INSERT INTO dst
SELECT *
FROM TABLE(extern(
   '{
     "type": "inline",
     "data": "a,b,1\nc,d,2\n"
    }',
	'{
	  "type": "csv",
	  "columns": ["x","y","z"],
	  "listDelimiter": null,
	  "findColumnsFromHeader": false,
	  "skipHeaderRows": 0
	 }',
	 '[
	   {"name": "x", "type": "STRING"},
	   {"name": "y", "type": "STRING"},
	   {"name": "z", "type": "LONG"}
	 ]'
))
ORDER BY 2
PARTITIONED BY ALL TIME
=== exception
ValidationException
=== error
Cannot have ORDER BY on an INSERT query, use CLUSTERED BY instead.
==============================================================
Converted from testInsertWithPartitionedByContainingInvalidGranularity()
=== case
INSERT with PARTITIONED BY containing an invalid granularity
=== SQL
INSERT INTO dst SELECT * FROM foo PARTITIONED BY 'invalid_granularity'
=== exception
SqlParseException
=== error
Encountered 'invalid_granularity' after PARTITIONED BY. Expected HOUR, DAY, MONTH, YEAR, ALL TIME, FLOOR function or TIME_FLOOR function
==============================================================
Converted from testInsertWithOrderBy()
=== case
INSERT with ORDER BY
=== SQL
INSERT INTO dst
SELECT *
FROM TABLE(extern(
   '{
     "type": "inline",
     "data": "a,b,1\nc,d,2\n"
    }',
	'{
	  "type": "csv",
	  "columns": ["x","y","z"],
	  "listDelimiter": null,
	  "findColumnsFromHeader": false,
	  "skipHeaderRows": 0
	 }',
	 '[
	   {"name": "x", "type": "STRING"},
	   {"name": "y", "type": "STRING"},
	   {"name": "z", "type": "LONG"}
	 ]'
))
ORDER BY 2
PARTITIONED BY ALL TIME
=== exception
ValidationException
=== error
Cannot have ORDER BY on an INSERT query, use CLUSTERED BY instead.
==============================================================
Converted from testInsertWithoutPartitionedBy()
=== case
INSERT without PARTITIONED BY
=== SQL
INSERT INTO dst
SELECT *
FROM TABLE(extern(
   '{
     "type": "inline",
     "data": "a,b,1\nc,d,2\n"
    }',
	'{
	  "type": "csv",
	  "columns": ["x","y","z"],
	  "listDelimiter": null,
	  "findColumnsFromHeader": false,
	  "skipHeaderRows": 0
	 }',
	 '[
	   {"name": "x", "type": "STRING"},
	   {"name": "y", "type": "STRING"},
	   {"name": "z", "type": "LONG"}
	 ]'
))
=== exception
ValidationException
=== error
INSERT statements must specify PARTITIONED BY clause explicitly
==============================================================
Converted from testInsertFromExternalProjectSort()
=== case
INSERT from external project sort
=== SQL
INSERT INTO dst
SELECT
  x || y AS xy,
  z
FROM TABLE(extern(
   '{
     "type": "inline",
     "data": "a,b,1\nc,d,2\n"
    }',
	'{
	  "type": "csv",
	  "columns": ["x","y","z"],
	  "listDelimiter": null,
	  "findColumnsFromHeader": false,
	  "skipHeaderRows": 0
	 }',
	 '[
	   {"name": "x", "type": "STRING"},
	   {"name": "y", "type": "STRING"},
	   {"name": "z", "type": "LONG"}
	 ]'
))
PARTITIONED BY ALL TIME
CLUSTERED BY 1, 2
=== targetSchema
xy VARCHAR
z BIGINT
=== resources
DATASOURCE/dst/WRITE
EXTERNAL/EXTERNAL/READ
=== plan
LogicalInsert(target=[dst], granularity=[AllGranularity])
  Clustered By: 1, 2  LogicalSort(sort0=[$0], sort1=[$1], dir0=[ASC], dir1=[ASC])
    LogicalProject(xy=[||($0, $1)], z=[$2])
      ExternalTableScan(dataSource=[{"type":"external","inputSource":{"type":"inline","data":"a,b,1\nc,d,2\n"},"inputFormat":{"type":"csv","columns":["x","y","z"],"listDelimiter":null,"findColumnsFromHeader":false,"skipHeaderRows":0},"signature":[{"name":"x","type":"STRING"},{"name":"y","type":"STRING"},{"name":"z","type":"LONG"}]}])
=== execPlan
{
  "type" : "query_controller",
  "id" : "query-dummyId",
  "spec" : {
    "query" : {
      "queryType" : "scan",
      "dataSource" : {
        "type" : "external",
        "inputSource" : {
          "type" : "inline",
          "data" : "a,b,1\nc,d,2\n"
        },
        "inputFormat" : {
          "type" : "csv",
          "columns" : [ "x", "y", "z" ],
          "listDelimiter" : null,
          "findColumnsFromHeader" : false,
          "skipHeaderRows" : 0
        },
        "signature" : [ {
          "name" : "x",
          "type" : "STRING"
        }, {
          "name" : "y",
          "type" : "STRING"
        }, {
          "name" : "z",
          "type" : "LONG"
        } ]
      },
      "intervals" : {
        "type" : "intervals",
        "intervals" : [ "-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z" ]
      },
      "virtualColumns" : [ {
        "type" : "expression",
        "name" : "v0",
        "expression" : "concat(\"x\",\"y\")",
        "outputType" : "STRING"
      } ],
      "resultFormat" : "compactedList",
      "orderBy" : [ {
        "columnName" : "v0",
        "order" : "ascending"
      }, {
        "columnName" : "z",
        "order" : "ascending"
      } ],
      "columns" : [ "v0", "z" ],
      "legacy" : false,
      "context" : {
        "finalize" : true,
        "msqSignature" : "[{\"name\":\"v0\",\"type\":\"STRING\"},{\"name\":\"z\",\"type\":\"LONG\"}]",
        "multiStageQuery" : true,
        "sqlInsertSegmentGranularity" : "{\"type\":\"all\"}",
        "sqlQueryId" : "dummyId"
      },
      "granularity" : {
        "type" : "all"
      }
    },
    "columnMappings" : [ {
      "queryColumn" : "v0",
      "outputColumn" : "xy"
    }, {
      "queryColumn" : "z",
      "outputColumn" : "z"
    } ],
    "destination" : {
      "type" : "dataSource",
      "dataSource" : "dst",
      "segmentGranularity" : {
        "type" : "all"
      }
    },
    "tuningConfig" : {
      "type" : "index_parallel",
      "partitionsSpec" : {
        "type" : "dynamic",
        "maxRowsPerSegment" : 3000000,
        "maxTotalRows" : null
      },
      "maxNumConcurrentSubTasks" : 1,
      "maxRetry" : 1,
      "maxRowsInMemory" : 100000
    }
  },
  "sqlQuery" : "INSERT INTO dst\nSELECT\n  x || y AS xy,\n  z\nFROM TABLE(extern(\n   '{\n     \"type\": \"inline\",\n     \"data\": \"a,b,1\\nc,d,2\\n\"\n    }',\n\t'{\n\t  \"type\": \"csv\",\n\t  \"columns\": [\"x\",\"y\",\"z\"],\n\t  \"listDelimiter\": null,\n\t  \"findColumnsFromHeader\": false,\n\t  \"skipHeaderRows\": 0\n\t }',\n\t '[\n\t   {\"name\": \"x\", \"type\": \"STRING\"},\n\t   {\"name\": \"y\", \"type\": \"STRING\"},\n\t   {\"name\": \"z\", \"type\": \"LONG\"}\n\t ]'\n))\nPARTITIONED BY ALL TIME\nCLUSTERED BY 1, 2",
  "sqlQueryContext" : {
    "maxParseExceptions" : 0,
    "msqSignature" : "[{\"name\":\"v0\",\"type\":\"STRING\"},{\"name\":\"z\",\"type\":\"LONG\"}]",
    "multiStageQuery" : true,
    "sqlInsertSegmentGranularity" : "{\"type\":\"all\"}",
    "sqlQueryId" : "dummyId"
  },
  "sqlTypeNames" : [ "VARCHAR", "BIGINT" ]
}
==============================================================
Converted from testInsertFromExternalAggregate()
INSERT with rollup
=== case
INSERT from external aggregate
=== SQL
INSERT INTO dst
SELECT
  x,
  SUM(z) AS sum_z,
  COUNT(*) AS cnt
FROM TABLE(extern(
   '{
     "type": "inline",
     "data": "a,b,1\nc,d,2\n"
    }',
	'{
	  "type": "csv",
	  "columns": ["x","y","z"],
	  "listDelimiter": null,
	  "findColumnsFromHeader": false,
	  "skipHeaderRows": 0
	 }',
	 '[
	   {"name": "x", "type": "STRING"},
	   {"name": "y", "type": "STRING"},
	   {"name": "z", "type": "LONG"}
	 ]'
))
GROUP BY 1
PARTITIONED BY ALL TIME
=== targetSchema
x VARCHAR
sum_z BIGINT
cnt BIGINT
=== resources
DATASOURCE/dst/WRITE
EXTERNAL/EXTERNAL/READ
=== plan
LogicalInsert(target=[dst], granularity=[AllGranularity])
  LogicalAggregate(group=[{0}], sum_z=[SUM($1)], cnt=[COUNT()])
    LogicalProject(x=[$0], z=[$2])
      ExternalTableScan(dataSource=[{"type":"external","inputSource":{"type":"inline","data":"a,b,1\nc,d,2\n"},"inputFormat":{"type":"csv","columns":["x","y","z"],"listDelimiter":null,"findColumnsFromHeader":false,"skipHeaderRows":0},"signature":[{"name":"x","type":"STRING"},{"name":"y","type":"STRING"},{"name":"z","type":"LONG"}]}])
=== execPlan
{
  "type" : "query_controller",
  "id" : "query-dummyId",
  "spec" : {
    "query" : {
      "queryType" : "groupBy",
      "dataSource" : {
        "type" : "external",
        "inputSource" : {
          "type" : "inline",
          "data" : "a,b,1\nc,d,2\n"
        },
        "inputFormat" : {
          "type" : "csv",
          "columns" : [ "x", "y", "z" ],
          "listDelimiter" : null,
          "findColumnsFromHeader" : false,
          "skipHeaderRows" : 0
        },
        "signature" : [ {
          "name" : "x",
          "type" : "STRING"
        }, {
          "name" : "y",
          "type" : "STRING"
        }, {
          "name" : "z",
          "type" : "LONG"
        } ]
      },
      "intervals" : {
        "type" : "intervals",
        "intervals" : [ "-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z" ]
      },
      "granularity" : {
        "type" : "all"
      },
      "dimensions" : [ {
        "type" : "default",
        "dimension" : "x",
        "outputName" : "d0",
        "outputType" : "STRING"
      } ],
      "aggregations" : [ {
        "type" : "longSum",
        "name" : "a0",
        "fieldName" : "z"
      }, {
        "type" : "count",
        "name" : "a1"
      } ],
      "limitSpec" : {
        "type" : "NoopLimitSpec"
      },
      "context" : {
        "finalize" : true,
        "msqSignature" : "[{\"name\":\"x\",\"type\":\"STRING\"},{\"name\":\"z\",\"type\":\"LONG\"}]",
        "multiStageQuery" : true,
        "sqlInsertSegmentGranularity" : "{\"type\":\"all\"}",
        "sqlQueryId" : "dummyId"
      }
    },
    "columnMappings" : [ {
      "queryColumn" : "d0",
      "outputColumn" : "x"
    }, {
      "queryColumn" : "a0",
      "outputColumn" : "sum_z"
    }, {
      "queryColumn" : "a1",
      "outputColumn" : "cnt"
    } ],
    "destination" : {
      "type" : "dataSource",
      "dataSource" : "dst",
      "segmentGranularity" : {
        "type" : "all"
      }
    },
    "tuningConfig" : {
      "type" : "index_parallel",
      "partitionsSpec" : {
        "type" : "dynamic",
        "maxRowsPerSegment" : 3000000,
        "maxTotalRows" : null
      },
      "maxNumConcurrentSubTasks" : 1,
      "maxRetry" : 1,
      "maxRowsInMemory" : 100000
    }
  },
  "sqlQuery" : "INSERT INTO dst\nSELECT\n  x,\n  SUM(z) AS sum_z,\n  COUNT(*) AS cnt\nFROM TABLE(extern(\n   '{\n     \"type\": \"inline\",\n     \"data\": \"a,b,1\\nc,d,2\\n\"\n    }',\n\t'{\n\t  \"type\": \"csv\",\n\t  \"columns\": [\"x\",\"y\",\"z\"],\n\t  \"listDelimiter\": null,\n\t  \"findColumnsFromHeader\": false,\n\t  \"skipHeaderRows\": 0\n\t }',\n\t '[\n\t   {\"name\": \"x\", \"type\": \"STRING\"},\n\t   {\"name\": \"y\", \"type\": \"STRING\"},\n\t   {\"name\": \"z\", \"type\": \"LONG\"}\n\t ]'\n))\nGROUP BY 1\nPARTITIONED BY ALL TIME",
  "sqlQueryContext" : {
    "maxParseExceptions" : 0,
    "msqSignature" : "[{\"name\":\"x\",\"type\":\"STRING\"},{\"name\":\"z\",\"type\":\"LONG\"}]",
    "multiStageQuery" : true,
    "sqlInsertSegmentGranularity" : "{\"type\":\"all\"}",
    "sqlQueryId" : "dummyId"
  },
  "sqlTypeNames" : [ "VARCHAR", "BIGINT", "BIGINT" ]
}
==============================================================
Converted from testInsertFromExternalAggregateAll()
INSERT with rollup into a single row (no GROUP BY exprs).
=== case
INSERT from external aggregate all
=== SQL
INSERT INTO dst
SELECT COUNT(*) AS cnt
FROM TABLE(extern(
   '{
     "type": "inline",
     "data": "a,b,1\nc,d,2\n"
    }',
	'{
	  "type": "csv",
	  "columns": ["x","y","z"],
	  "listDelimiter": null,
	  "findColumnsFromHeader": false,
	  "skipHeaderRows": 0
	 }',
	 '[
	   {"name": "x", "type": "STRING"},
	   {"name": "y", "type": "STRING"},
	   {"name": "z", "type": "LONG"}
	 ]'
))
PARTITIONED BY ALL TIME
=== targetSchema
cnt BIGINT
=== resources
DATASOURCE/dst/WRITE
EXTERNAL/EXTERNAL/READ
=== plan
LogicalInsert(target=[dst], granularity=[AllGranularity])
  LogicalAggregate(group=[{}], cnt=[COUNT()])
    LogicalProject($f0=[0])
      ExternalTableScan(dataSource=[{"type":"external","inputSource":{"type":"inline","data":"a,b,1\nc,d,2\n"},"inputFormat":{"type":"csv","columns":["x","y","z"],"listDelimiter":null,"findColumnsFromHeader":false,"skipHeaderRows":0},"signature":[{"name":"x","type":"STRING"},{"name":"y","type":"STRING"},{"name":"z","type":"LONG"}]}])
=== execPlan
{
  "type" : "query_controller",
  "id" : "query-dummyId",
  "spec" : {
    "query" : {
      "queryType" : "groupBy",
      "dataSource" : {
        "type" : "external",
        "inputSource" : {
          "type" : "inline",
          "data" : "a,b,1\nc,d,2\n"
        },
        "inputFormat" : {
          "type" : "csv",
          "columns" : [ "x", "y", "z" ],
          "listDelimiter" : null,
          "findColumnsFromHeader" : false,
          "skipHeaderRows" : 0
        },
        "signature" : [ {
          "name" : "x",
          "type" : "STRING"
        }, {
          "name" : "y",
          "type" : "STRING"
        }, {
          "name" : "z",
          "type" : "LONG"
        } ]
      },
      "intervals" : {
        "type" : "intervals",
        "intervals" : [ "-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z" ]
      },
      "granularity" : {
        "type" : "all"
      },
      "dimensions" : [ ],
      "aggregations" : [ {
        "type" : "count",
        "name" : "a0"
      } ],
      "limitSpec" : {
        "type" : "NoopLimitSpec"
      },
      "context" : {
        "finalize" : true,
        "msqSignature" : "[{\"name\":\"v0\",\"type\":\"LONG\"}]",
        "multiStageQuery" : true,
        "sqlInsertSegmentGranularity" : "{\"type\":\"all\"}",
        "sqlQueryId" : "dummyId"
      }
    },
    "columnMappings" : [ {
      "queryColumn" : "a0",
      "outputColumn" : "cnt"
    } ],
    "destination" : {
      "type" : "dataSource",
      "dataSource" : "dst",
      "segmentGranularity" : {
        "type" : "all"
      }
    },
    "tuningConfig" : {
      "type" : "index_parallel",
      "partitionsSpec" : {
        "type" : "dynamic",
        "maxRowsPerSegment" : 3000000,
        "maxTotalRows" : null
      },
      "maxNumConcurrentSubTasks" : 1,
      "maxRetry" : 1,
      "maxRowsInMemory" : 100000
    }
  },
  "sqlQuery" : "INSERT INTO dst\nSELECT COUNT(*) AS cnt\nFROM TABLE(extern(\n   '{\n     \"type\": \"inline\",\n     \"data\": \"a,b,1\\nc,d,2\\n\"\n    }',\n\t'{\n\t  \"type\": \"csv\",\n\t  \"columns\": [\"x\",\"y\",\"z\"],\n\t  \"listDelimiter\": null,\n\t  \"findColumnsFromHeader\": false,\n\t  \"skipHeaderRows\": 0\n\t }',\n\t '[\n\t   {\"name\": \"x\", \"type\": \"STRING\"},\n\t   {\"name\": \"y\", \"type\": \"STRING\"},\n\t   {\"name\": \"z\", \"type\": \"LONG\"}\n\t ]'\n))\nPARTITIONED BY ALL TIME",
  "sqlQueryContext" : {
    "maxParseExceptions" : 0,
    "msqSignature" : "[{\"name\":\"v0\",\"type\":\"LONG\"}]",
    "multiStageQuery" : true,
    "sqlInsertSegmentGranularity" : "{\"type\":\"all\"}",
    "sqlQueryId" : "dummyId"
  },
  "sqlTypeNames" : [ "BIGINT" ]
}
==============================================================
Converted from testInsertWithInvalidSelectStatement()
=== case
INSERT with invalid SELECT statement
=== SQL
INSERT INTO t
SELECT channel, added as count -- count is a keyword
FROM foo
PARTITIONED BY ALL
=== exception
SqlParseException
=== error
Encountered "as count" at line 2, column 23.
**
==============================================================
Converted from testInsertWithUnnamedColumnInSelectStatement()
=== case
INSERT with unnamed column in SELECT statement
=== SQL
INSERT INTO t
SELECT dim1, dim2 || '-lol'
FROM foo
PARTITIONED BY ALL
=== exception
ValidationException
=== error
Cannot ingest expressions that do not have an alias or columns with names like EXPR$[digit].
E.g. if you are ingesting "func(X)", then you can rewrite it as "func(X) as myColumn"
==============================================================
Converted from testInsertWithInvalidColumnNameInIngest()
=== case
INSERT with unnamed column in SELECT statement
=== SQL
INSERT INTO t
SELECT dim1, dim2 || '-lol'
FROM foo
PARTITIONED BY ALL
=== exception
ValidationException
=== error
Cannot ingest expressions that do not have an alias or columns with names like EXPR$[digit].
E.g. if you are ingesting "func(X)", then you can rewrite it as "func(X) as myColumn"
==============================================================
Converted from testInsertWithUnnamedColumnInNestedSelectStatement()
=== case
INSERT with unnamed column in nested SELECT statement
=== SQL
INSERT INTO test
SELECT __time, *
FROM
  (SELECT __time, LOWER(dim1) FROM foo)
PARTITIONED BY ALL TIME
=== exception
ValidationException
=== error
Cannot ingest expressions that do not have an alias or columns with names like EXPR$[digit].
E.g. if you are ingesting "func(X)", then you can rewrite it as "func(X) as myColumn"
